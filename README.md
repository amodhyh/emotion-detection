# Emotion Detection in Sinhala Social Media Texts

## Project overview

This repository implements an end-to-end pipeline for emotion detection in Sinhala social media text. It combines a pre-trained SinBERT encoder with a small CNN classifier (SinBERT + CNN) to capture both contextual embeddings and local n-gram features.

Primary artifacts
- preprocessing.ipynb — data cleaning, agreement filtering and dataset creation (saved to ./processed/processed_dataset.csv)
- Model.ipynb — model definition, training loop, hyperparameter tuning and evaluation
- processed/processed_dataset.csv — cleaned dataset used for experiments
- SS/train and validation loss.png — sample training diagnostics
- requirements.txt — python package dependencies

## Quick start

1. Create and activate a virtual environment (recommended):

   python -m venv .venv
   .\.venv\Scripts\activate

2. Install dependencies:

   pip install -r requirements.txt

3. If running in Jupyter and you see tqdm / notebook progress bar errors, install/enable widgets:

   pip install ipywidgets
   jupyter nbextension enable --py widgetsnbextension

4. Run notebooks in order:
   - preprocessing.ipynb
   - Model.ipynb

## Data

- Raw source: raw/Dataset.csv
- Processed dataset written to: processed/processed_dataset.csv
- Labels are generated by keeping only rows where both annotators agreed.

## Model architecture

High level pipeline (ASCII diagram):

 text -> tokenizer -> SinBERT encoder -> contextual embeddings -> CNN feature extraction -> dropout -> dense -> softmax -> prediction

Detailed model block (SinBERT + CNN):

 [Input tokens]
     |
 [SinBERT (pretrained) — outputs token embeddings / pooled output]
     |
 [CNN branches: multiple 1D convs with kernel sizes k1,k2,k3]
     |             (each branch: Conv1D -> ReLU -> MaxPool)
     +------------+
                  |
             Concatenate
                  |
              Dropout
                  |
           Fully connected
                  |
               Softmax

Notes:
- SinBERT provides contextualized token embeddings. The CNN layers operate on token-level embeddings to capture local n-gram patterns.
- The model may show a warning at load time: "Some weights of RobertaModel were not initialized from the model checkpoint ... pooler.dense.*" — this indicates the pre-trained checkpoint did not include a pooler or that pooler weights are randomly initialized. Fine-tune the model to initialize those weights properly for downstream tasks.

## Where AdamW is used

The recommended optimizer in the notebooks is AdamW (from transformers or torch.optim). Example usage:

    from transformers import AdamW
    optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)

Look for the optimizer instantiation in Model.ipynb; AdamW is applied to model.parameters(). If you need per-parameter weight decay masking (no decay for biases and LayerNorm), consider grouping parameters before creating the optimizer (common in transformer fine-tuning).

Example parameter grouping:

    no_decay = ["bias", "LayerNorm.weight"]
    grouped_parameters = [
        {"params": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], "weight_decay": 0.01},
        {"params": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], "weight_decay": 0.0}
    ]
    optimizer = AdamW(grouped_parameters, lr=2e-5)

## Training, cross-validation and hyperparameter tuning

- Implement k-fold cross-validation (e.g., StratifiedKFold from scikit-learn) to obtain robust estimates.
- Save training and validation loss per epoch and plot them to visually check overfitting/underfitting.

Suggested training loop structure (high level):

1. For each fold:
   - Split dataset into train/validation
   - Instantiate a fresh model (to avoid leaking weights across folds)
   - Create optimizer (AdamW) and scheduler (optional)
   - Train for N epochs recording train & validation losses
   - Evaluate on validation set, store metrics
2. Aggregate fold metrics and plot mean +/- std performance

Plotting suggestions:
- Training vs validation loss curves (per fold and average across folds)
- Bar chart of metrics (accuracy/F1) across folds
- Confusion matrix for best model

## Evaluation metrics

- Use accuracy, precision, recall, F1-score (macro and/or weighted depending on class imbalance).
- Save and print best result across hyperparameter combinations. The notebooks already collect results during tuning; add a final print/sort by metric to display top configurations.

## Troubleshooting & common errors

- tqdm / ipywidgets ImportError: install ipywidgets and enable the extension (see Quick start).
- TypeError new(): invalid data type 'str' when converting labels to tensors: ensure dataset labels are integer-encoded (0..C-1) before creating torch.tensor(..., dtype=torch.long). Example fix in dataset __getitem__:

    label = int(self.label_encodings[item])  # or self.label2id[label_str]
    return { ..., 'labels': torch.tensor(label, dtype=torch.long) }

- Pooler weights warning: benign but indicates pooler parameters were not included in the checkpoint; fine-tune the model and the warning will be resolved when you train for the downstream task.

- If running DataLoader and you hit issues related to dataset indexing, ensure __len__ and __getitem__ are implemented correctly and that __getitem__ returns tensors, not strings.

## Output and artifacts

- Trained model checkpoints (save best checkpoint per fold)
- Training diagnostics: loss curves saved under SS/ or results/ (example: SS/train and validation loss.png)
- CSV/JSON containing per-fold metrics and hyperparameter settings

## Reproducing hyperparameter search

- The notebooks implement an exhaustive sweep across a small hyperparameter grid (embedding_dim, num_filters, kernel_sizes, dropout). To run a reproducible search:
  - Fix RNG seeds for numpy, torch and transformers
  - Use small pilot runs to narrow the search space
  - Save the results list and serialize to CSV for later analysis

## Suggestions for improvements

- Use grouped weight decay when creating AdamW to avoid decaying bias and LayerNorm parameters.
- Consider using a learning rate scheduler with warmup (transformers' get_linear_schedule_with_warmup).
- Use mixed precision training (torch.cuda.amp) to speed up training and reduce memory usage.
- If the dataset is small, use data augmentation (back-translation or synonym replacement in Sinhala) or cross-lingual transfer.

## Example commands

Run preprocessing notebook:

   jupyter nbconvert --to notebook --execute preprocessing.ipynb --inplace

Run Model notebook (this runs training which may be long):

   jupyter nbconvert --to notebook --execute Model.ipynb --inplace

## Project credits
- Authors: 2021/E/045 and 2021/E053
- Pretrained model: NLPC-UOM/SinBERT-large (Hugging Face)

## License
Add your license information here.


