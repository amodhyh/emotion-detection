{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-07T14:12:13.004364Z",
     "start_time": "2025-09-07T14:11:10.912530Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from torch.optim import AdamW\n",
    "from transformers import RobertaModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA toolkit version:\", torch.version.cuda)\n",
    "print(\"Device name:\", torch.cuda.get_device_name(0))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.7.1+cu118\n",
      "CUDA available: True\n",
      "CUDA toolkit version: 11.8\n",
      "Device name: NVIDIA GeForce RTX 3050 Laptop GPU\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T14:13:34.280024Z",
     "start_time": "2025-09-07T14:13:18.202864Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pipe = pipeline(\"fill-mask\", model=\"NLPC-UOM/SinBERT-large\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"NLPC-UOM/SinBERT-large\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"NLPC-UOM/SinBERT-large\")"
   ],
   "id": "b474cd6d67b75623",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T14:14:08.172615Z",
     "start_time": "2025-09-07T14:14:07.732807Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)"
   ],
   "id": "f34b77002331410",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T14:14:11.357815Z",
     "start_time": "2025-09-07T14:14:11.049656Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = pd.read_csv('./processed/processed_dataset.csv')\n",
    "\n",
    "# After loading df and before defining criterion\n",
    "labels = df['label'].values\n",
    "class_sample_count = np.array([np.sum(labels == t) for t in np.unique(labels)])\n",
    "weight = 1. / class_sample_count\n",
    "\n",
    "#converts numpy array of weights to torch tensor and move it to the same device as the model\n",
    "class_weights = torch.tensor(weight, dtype=torch.float).to(device)\n",
    "\n",
    "# Use in loss function\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer= AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)"
   ],
   "id": "7b732447ac9d0c11",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T14:14:14.068817Z",
     "start_time": "2025-09-07T14:14:14.063042Z"
    }
   },
   "cell_type": "code",
   "source": "class_sample_count",
   "id": "faffd825da3ebc6d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([502, 540, 504, 606, 507, 496])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Creating the DataSet Class\n",
   "id": "57edb3db029fe62b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "the class wich represent the data.the dataloder will use this class to load the data one by one or in batches.it relies on 2 methods.\n",
    "\n",
    "- `__len__`    - returns the length of the dataset\n",
    "- `__getitem__` - returns the item at the given index\n",
    "Also it has the constructor which takes the dataframe, tokenizer and max_length as input parameters.\n",
    "- `__init__`- constructor to initialize the dataset with a dataframe, tokenizer, and maximum sequence length.\n",
    "    - `self.text`,`self.label` - store the raw sinhala texts and labels from the dataframe.\n",
    "    - `self.tokenizer` - stores the tokenizer object.\n",
    "    - `self.max_length` - stores the maximum length of the input sequence.all the sentences will be padded or truncated to this length."
   ],
   "id": "f7f199f83b3fd68a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T14:14:17.996669Z",
     "start_time": "2025-09-07T14:14:17.991622Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SinhalaEmotionDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
    "        self.texts = texts\n",
    "        # Create label-to-index mapping\n",
    "        self.label2idx = {label: idx for idx, label in enumerate(sorted(set(labels)))}\n",
    "        self.labels = [self.label2idx[label] for label in labels]\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        text = str(self.texts[item])\n",
    "        label = self.labels[item]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ],
   "id": "b0d897f202c10d21",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Dataloader",
   "id": "22afb248ea02fb4b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T14:14:21.259606Z",
     "start_time": "2025-09-07T14:14:21.147245Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_train,df_test=train_test_split(df,test_size=0.2,random_state=42,stratify=df['label'])#stratify is used to maintain the class distribution in the train and test sets(emotion percentages are same in both sets).\n",
    "\n",
    "#Dataset and Dataloader\n",
    "BATCH_SIZE = 16\n",
    "# Instantiate the custom dataset for each split\n",
    "train_dataset = SinhalaEmotionDataset(\n",
    "    texts=df_train['text'].tolist(),\n",
    "    labels=df_train['label'].tolist(),\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "test_dataset = SinhalaEmotionDataset(\n",
    "    texts=df_test['text'].tolist(),\n",
    "    labels=df_test['label'].tolist(),\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "# Set up the DataLoader for each split\n",
    "train_data_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_data_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "print(\"Data Loaders for training, validation, and testing are ready.\")"
   ],
   "id": "9bc8657e18356f34",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loaders for training, validation, and testing are ready.\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Building the Model Architecture\n",
   "id": "8b20b07c5067221e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "every pytorch model should inherit from `nn.Module` class. The model should have the following methods.\n",
    "- `__init__` - constructor to initialize the model with a pre-trained BERT model and a CNN layer.\n",
    "    - `self.bert` - stores the pre-trained BERT model.\n",
    "    - `self.cnn` - stores the CNN layer.\n",
    "    - `self.relu` - stores the ReLU activation function.\n",
    "    - `self.pool` - stores the adaptive max pooling layer."
   ],
   "id": "65a3d2b79b4ea8c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T14:14:26.593448Z",
     "start_time": "2025-09-07T14:14:26.582159Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SinbertCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    SinbertCNN combines a RoBERTa-based encoder with CNN layers for emotion classification.\n",
    "    Args:\n",
    "        num_labels (int): Number of output classes.\n",
    "        embedding_dim (int): Dimensionality of SinBERT embeddings.\n",
    "        num_filters (int): Number of filters per CNN layer.\n",
    "        kernel_sizes (list): List of kernel sizes for CNN layers.\n",
    "        dropout_rate (float): Dropout rate for regularization.\n",
    "        pretrained_model_name (str): Name of the pretrained RoBERTa model.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_labels, embedding_dim=1024, num_filters=100, kernel_sizes=None, dropout_rate=0.5, pretrained_model_name=\"NLPC-UOM/SinBERT-large\"):\n",
    "        super(SinbertCNN, self).__init__()\n",
    "        if kernel_sizes is None:\n",
    "            kernel_sizes = [3, 4, 5]\n",
    "        self.sinbert = RobertaModel.from_pretrained(pretrained_model_name)\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(in_channels=embedding_dim,\n",
    "                      out_channels=num_filters,\n",
    "                      kernel_size=k)\n",
    "            for k in kernel_sizes\n",
    "        ])\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(num_filters * len(kernel_sizes), num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.sinbert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        embeddings = outputs.last_hidden_state\n",
    "        x = embeddings.permute(0, 2, 1)\n",
    "        conv_outputs = [torch.relu(conv(x)) for conv in self.convs]\n",
    "        pooled_outputs = [torch.max(conv_output, dim=2)[0] for conv_output in conv_outputs]\n",
    "        concatenated_output = torch.cat(pooled_outputs, dim=1)\n",
    "        dropout_output = self.dropout(concatenated_output)\n",
    "        logits = self.fc(dropout_output)\n",
    "        return logits"
   ],
   "id": "9b44de63a9e37719",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training the Model",
   "id": "8059f8724222baac"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T14:35:37.089199Z",
     "start_time": "2025-09-07T14:35:37.081610Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, device, epochs=3):\n",
    "    train_losses, train_accuracies = [], []\n",
    "    val_losses, val_accuracies = [], []\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss, correct, total = 0.0, 0, 0\n",
    "        for batch in train_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * input_ids.size(0)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            correct += (preds == labels).float().sum().item()\n",
    "            total += labels.size(0)\n",
    "        epoch_loss = running_loss / total\n",
    "        epoch_acc = correct / total\n",
    "        train_losses.append(epoch_loss)\n",
    "        train_accuracies.append(epoch_acc)\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_running_loss, val_correct, val_total = 0.0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_running_loss += loss.item() * input_ids.size(0)\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                val_correct += (preds == labels).float().sum().item()\n",
    "                val_total += labels.size(0)\n",
    "        val_losses.append(val_running_loss / val_total)\n",
    "        val_accuracies.append(val_correct / val_total)\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_acc:.4f}, Val Loss: {val_losses[-1]:.4f}, Val Acc: {val_accuracies[-1]:.4f}\")\n",
    "    return train_losses, train_accuracies, val_losses, val_accuracies\n",
    "\n",
    "def evaluate_metrics(model, data_loader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted')\n",
    "    return acc, precision, recall, f1\n",
    "\n",
    "# Usage:\n",
    "# Use your validation or test loader for proper evaluation\n",
    "# acc, precision, recall, f1 = evaluate_metrics(model, val_loader, device)"
   ],
   "id": "17802b65b3b115ae",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T15:45:53.417100Z",
     "start_time": "2025-09-05T15:45:49.901582Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install ipywidgets",
   "id": "55d810a99b5c8648",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipywidgets in f:\\uoje\\research\\.venv\\lib\\site-packages (8.1.7)\n",
      "Requirement already satisfied: comm>=0.1.3 in f:\\uoje\\research\\.venv\\lib\\site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in f:\\uoje\\research\\.venv\\lib\\site-packages (from ipywidgets) (9.4.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in f:\\uoje\\research\\.venv\\lib\\site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in f:\\uoje\\research\\.venv\\lib\\site-packages (from ipywidgets) (4.0.14)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in f:\\uoje\\research\\.venv\\lib\\site-packages (from ipywidgets) (3.0.15)\n",
      "Requirement already satisfied: colorama in f:\\uoje\\research\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n",
      "Requirement already satisfied: decorator in f:\\uoje\\research\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in f:\\uoje\\research\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in f:\\uoje\\research\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in f:\\uoje\\research\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in f:\\uoje\\research\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.51)\n",
      "Requirement already satisfied: pygments>=2.4.0 in f:\\uoje\\research\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (2.19.2)\n",
      "Requirement already satisfied: stack_data in f:\\uoje\\research\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: wcwidth in f:\\uoje\\research\\.venv\\lib\\site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in f:\\uoje\\research\\.venv\\lib\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: executing>=1.2.0 in f:\\uoje\\research\\.venv\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in f:\\uoje\\research\\.venv\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in f:\\uoje\\research\\.venv\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Hyperparameter tuning",
   "id": "3175add2ad6e3282"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T14:14:57.706639Z",
     "start_time": "2025-09-07T14:14:56.923437Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Python\n",
    "from transformers import AutoConfig\n",
    "\n",
    "config = AutoConfig.from_pretrained(\"NLPC-UOM/SinBERT-large\")\n",
    "print(\"Hidden size:\", config.hidden_size)  # Use this value for embedding_dim"
   ],
   "id": "994d7c6308828cbc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden size: 768\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T14:15:01.080257Z",
     "start_time": "2025-09-07T14:15:01.072658Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "def train_model_with_early_stopping(model, train_loader, val_loader, criterion, optimizer, device, epochs=10, patience=2):\n",
    "    train_losses, train_accuracies = [], []\n",
    "    val_losses, val_accuracies = [], []\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss, correct, total = 0.0, 0, 0\n",
    "        for batch in train_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * input_ids.size(0)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            correct += (preds == labels).float().sum().item()\n",
    "            total += labels.size(0)\n",
    "        epoch_loss = running_loss / total\n",
    "        epoch_acc = correct / total\n",
    "        train_losses.append(epoch_loss)\n",
    "        train_accuracies.append(epoch_acc)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_running_loss, val_correct, val_total = 0.0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_running_loss += loss.item() * input_ids.size(0)\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                val_correct += (preds == labels).float().sum().item()\n",
    "                val_total += labels.size(0)\n",
    "        val_loss = val_running_loss / val_total\n",
    "        val_acc = val_correct / val_total\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_acc)\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "        # Early stopping check\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            best_model_state = model.state_dict()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                model.load_state_dict(best_model_state)\n",
    "                break\n",
    "\n",
    "    return train_losses, train_accuracies, val_losses, val_accuracies"
   ],
   "id": "e9fac780a443653b",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "613d4e9aa904caad"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Hyper parameter Tuning",
   "id": "a1cd57101f51d002"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-06T19:05:17.417625Z",
     "start_time": "2025-09-05T17:25:43.284521Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import itertools\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'embedding_dim': [768],\n",
    "    'num_filters': [50, 100],\n",
    "    'kernel_sizes': [[3, 4, 5], [2, 3, 4, 5]],\n",
    "    'dropout_rate': [0.2, 0.3],\n",
    "}\n",
    "\n",
    "all_combinations = list(itertools.product(\n",
    "    param_grid['embedding_dim'],\n",
    "    param_grid['num_filters'],\n",
    "    param_grid['kernel_sizes'],\n",
    "    param_grid['dropout_rate']\n",
    "))\n",
    "\n",
    "# Hyperparameter tuning loop with status bar\n",
    "results = []\n",
    "num_labels = len(df['label'].unique())\n",
    "\n",
    "for emb_dim, num_filt, kernels, drop in tqdm(all_combinations, desc='Hyperparameter Tuning'):\n",
    "    model = SinbertCNN(\n",
    "        num_labels=num_labels,\n",
    "        embedding_dim=emb_dim,\n",
    "        num_filters=num_filt,\n",
    "        kernel_sizes=kernels,\n",
    "        dropout_rate=drop\n",
    "    ).to(device)\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "    train_model_with_early_stopping(\n",
    "        model=model,\n",
    "        train_loader=train_data_loader,\n",
    "        val_loader=test_data_loader,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        device=device,\n",
    "        epochs=10,      # Set higher epochs\n",
    "        patience=2      # Early stopping patience\n",
    "    )\n",
    "    acc, precision, recall, f1 = evaluate_metrics(model, test_data_loader, device)\n",
    "    results.append({\n",
    "        'params': (emb_dim, num_filt, kernels, drop),\n",
    "        'accuracy': acc,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    })\n",
    "\n",
    "# After hyperparameter tuning loop\n",
    "best_result = max(results, key=lambda x: x['accuracy'])\n",
    "print('Best hyperparameters:', best_result['params'])\n",
    "print('Best accuracy:', best_result['accuracy'])\n",
    "print('Best precision:', best_result['precision'])\n",
    "print('Best recall:', best_result['recall'])\n",
    "print('Best F1:', best_result['f1'])\n",
    "\n",
    "# Plot performance metrics for all hyperparameter combinations\n",
    "accuracies = [r['accuracy'] for r in results]\n",
    "precisions = [r['precision'] for r in results]\n",
    "recalls = [r['recall'] for r in results]\n",
    "f1s = [r['f1'] for r in results]\n",
    "labels = [str(r['params']) for r in results]\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(labels, accuracies, marker='o', label='Accuracy')\n",
    "plt.plot(labels, precisions, marker='o', label='Precision')\n",
    "plt.plot(labels, recalls, marker='o', label='Recall')\n",
    "plt.plot(labels, f1s, marker='o', label='F1 Score')\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel('Hyperparameter Combination')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Performance Metrics Across Hyperparameter Combinations')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "d2dbe2d0be0fc4a4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Hyperparameter Tuning:   0%|          | 0/16 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6e06427bea6a4805852be9fc3937775c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at NLPC-UOM/SinBERT-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Train Loss: 1.5702, Train Acc: 0.3641, Val Loss: 1.1764, Val Acc: 0.5895\n",
      "Epoch 2/10 - Train Loss: 0.9968, Train Acc: 0.6375, Val Loss: 0.9436, Val Acc: 0.6704\n",
      "Epoch 3/10 - Train Loss: 0.6794, Train Acc: 0.7575, Val Loss: 0.8488, Val Acc: 0.7100\n",
      "Epoch 4/10 - Train Loss: 0.4553, Train Acc: 0.8427, Val Loss: 0.8647, Val Acc: 0.7147\n",
      "Epoch 5/10 - Train Loss: 0.3057, Train Acc: 0.8970, Val Loss: 0.9052, Val Acc: 0.7132\n",
      "Early stopping at epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at NLPC-UOM/SinBERT-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Train Loss: 1.7405, Train Acc: 0.3011, Val Loss: 1.3050, Val Acc: 0.5499\n",
      "Epoch 2/10 - Train Loss: 1.1804, Train Acc: 0.5475, Val Loss: 0.9473, Val Acc: 0.6624\n",
      "Epoch 3/10 - Train Loss: 0.8246, Train Acc: 0.7092, Val Loss: 0.8487, Val Acc: 0.6926\n",
      "Epoch 4/10 - Train Loss: 0.5816, Train Acc: 0.8015, Val Loss: 0.8326, Val Acc: 0.7005\n",
      "Epoch 5/10 - Train Loss: 0.3900, Train Acc: 0.8772, Val Loss: 0.8744, Val Acc: 0.7147\n",
      "Epoch 6/10 - Train Loss: 0.2794, Train Acc: 0.9168, Val Loss: 0.9491, Val Acc: 0.6989\n",
      "Early stopping at epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at NLPC-UOM/SinBERT-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Train Loss: 1.5655, Train Acc: 0.3784, Val Loss: 1.1591, Val Acc: 0.5990\n",
      "Epoch 2/10 - Train Loss: 0.9897, Train Acc: 0.6335, Val Loss: 0.8775, Val Acc: 0.6815\n",
      "Epoch 3/10 - Train Loss: 0.6621, Train Acc: 0.7611, Val Loss: 0.8094, Val Acc: 0.7211\n",
      "Epoch 4/10 - Train Loss: 0.4240, Train Acc: 0.8435, Val Loss: 0.8722, Val Acc: 0.7116\n",
      "Epoch 5/10 - Train Loss: 0.2700, Train Acc: 0.9073, Val Loss: 0.9079, Val Acc: 0.7227\n",
      "Early stopping at epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at NLPC-UOM/SinBERT-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Train Loss: 1.7107, Train Acc: 0.2983, Val Loss: 1.2547, Val Acc: 0.5769\n",
      "Epoch 2/10 - Train Loss: 1.1339, Train Acc: 0.5741, Val Loss: 0.9595, Val Acc: 0.6403\n",
      "Epoch 3/10 - Train Loss: 0.7825, Train Acc: 0.7199, Val Loss: 0.8486, Val Acc: 0.6783\n",
      "Epoch 4/10 - Train Loss: 0.5493, Train Acc: 0.8067, Val Loss: 0.8791, Val Acc: 0.6989\n",
      "Epoch 5/10 - Train Loss: 0.3870, Train Acc: 0.8625, Val Loss: 0.8286, Val Acc: 0.7068\n",
      "Epoch 6/10 - Train Loss: 0.2400, Train Acc: 0.9267, Val Loss: 0.9201, Val Acc: 0.7021\n",
      "Epoch 7/10 - Train Loss: 0.1675, Train Acc: 0.9485, Val Loss: 0.9901, Val Acc: 0.6878\n",
      "Early stopping at epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at NLPC-UOM/SinBERT-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Train Loss: 1.5675, Train Acc: 0.3657, Val Loss: 1.1683, Val Acc: 0.6086\n",
      "Epoch 2/10 - Train Loss: 0.9809, Train Acc: 0.6387, Val Loss: 0.9346, Val Acc: 0.6719\n",
      "Epoch 3/10 - Train Loss: 0.6518, Train Acc: 0.7694, Val Loss: 0.8680, Val Acc: 0.6894\n",
      "Epoch 4/10 - Train Loss: 0.4144, Train Acc: 0.8554, Val Loss: 0.8471, Val Acc: 0.7021\n",
      "Epoch 5/10 - Train Loss: 0.2819, Train Acc: 0.9085, Val Loss: 0.8940, Val Acc: 0.7195\n",
      "Epoch 6/10 - Train Loss: 0.1695, Train Acc: 0.9473, Val Loss: 0.8961, Val Acc: 0.7258\n",
      "Early stopping at epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at NLPC-UOM/SinBERT-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Train Loss: 1.6738, Train Acc: 0.3308, Val Loss: 1.2082, Val Acc: 0.5800\n",
      "Epoch 2/10 - Train Loss: 1.0657, Train Acc: 0.6042, Val Loss: 0.9303, Val Acc: 0.6751\n",
      "Epoch 3/10 - Train Loss: 0.7191, Train Acc: 0.7429, Val Loss: 0.8974, Val Acc: 0.6878\n",
      "Epoch 4/10 - Train Loss: 0.5051, Train Acc: 0.8269, Val Loss: 0.9304, Val Acc: 0.6957\n",
      "Epoch 5/10 - Train Loss: 0.3383, Train Acc: 0.8867, Val Loss: 0.8829, Val Acc: 0.7116\n",
      "Epoch 6/10 - Train Loss: 0.2218, Train Acc: 0.9251, Val Loss: 0.9333, Val Acc: 0.7036\n",
      "Epoch 7/10 - Train Loss: 0.1405, Train Acc: 0.9560, Val Loss: 1.0517, Val Acc: 0.7068\n",
      "Early stopping at epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at NLPC-UOM/SinBERT-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Train Loss: 1.5664, Train Acc: 0.3534, Val Loss: 1.1601, Val Acc: 0.6022\n",
      "Epoch 2/10 - Train Loss: 0.9551, Train Acc: 0.6616, Val Loss: 0.9033, Val Acc: 0.6910\n",
      "Epoch 3/10 - Train Loss: 0.6293, Train Acc: 0.7714, Val Loss: 0.8218, Val Acc: 0.7036\n",
      "Epoch 4/10 - Train Loss: 0.3953, Train Acc: 0.8613, Val Loss: 0.8285, Val Acc: 0.7242\n",
      "Epoch 5/10 - Train Loss: 0.2570, Train Acc: 0.9136, Val Loss: 0.9042, Val Acc: 0.7227\n",
      "Early stopping at epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at NLPC-UOM/SinBERT-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Train Loss: 1.7008, Train Acc: 0.3102, Val Loss: 1.2318, Val Acc: 0.5705\n",
      "Epoch 2/10 - Train Loss: 1.0868, Train Acc: 0.5852, Val Loss: 0.9363, Val Acc: 0.6339\n",
      "Epoch 3/10 - Train Loss: 0.7542, Train Acc: 0.7235, Val Loss: 0.8447, Val Acc: 0.7005\n",
      "Epoch 4/10 - Train Loss: 0.5157, Train Acc: 0.8181, Val Loss: 0.8262, Val Acc: 0.7147\n",
      "Epoch 5/10 - Train Loss: 0.3455, Train Acc: 0.8819, Val Loss: 0.8694, Val Acc: 0.7052\n",
      "Epoch 6/10 - Train Loss: 0.2248, Train Acc: 0.9303, Val Loss: 0.9375, Val Acc: 0.6957\n",
      "Early stopping at epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at NLPC-UOM/SinBERT-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [50, 1024, 3], expected input[16, 768, 128] to have 1024 channels, but got 768 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mRuntimeError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[13]\u001B[39m\u001B[32m, line 31\u001B[39m\n\u001B[32m     23\u001B[39m model = SinbertCNN(\n\u001B[32m     24\u001B[39m     num_labels=num_labels,\n\u001B[32m     25\u001B[39m     embedding_dim=emb_dim,\n\u001B[32m   (...)\u001B[39m\u001B[32m     28\u001B[39m     dropout_rate=drop\n\u001B[32m     29\u001B[39m ).to(device)\n\u001B[32m     30\u001B[39m optimizer = AdamW(model.parameters(), lr=\u001B[32m2e-5\u001B[39m, weight_decay=\u001B[32m0.01\u001B[39m)\n\u001B[32m---> \u001B[39m\u001B[32m31\u001B[39m \u001B[43mtrain_model_with_early_stopping\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     32\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     33\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtrain_data_loader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     34\u001B[39m \u001B[43m    \u001B[49m\u001B[43mval_loader\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtest_data_loader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     35\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     36\u001B[39m \u001B[43m    \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m=\u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     37\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     38\u001B[39m \u001B[43m    \u001B[49m\u001B[43mepochs\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m10\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m      \u001B[49m\u001B[38;5;66;43;03m# Set higher epochs\u001B[39;49;00m\n\u001B[32m     39\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpatience\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m2\u001B[39;49m\u001B[43m      \u001B[49m\u001B[38;5;66;43;03m# Early stopping patience\u001B[39;49;00m\n\u001B[32m     40\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     41\u001B[39m acc, precision, recall, f1 = evaluate_metrics(model, test_data_loader, device)\n\u001B[32m     42\u001B[39m results.append({\n\u001B[32m     43\u001B[39m     \u001B[33m'\u001B[39m\u001B[33mparams\u001B[39m\u001B[33m'\u001B[39m: (emb_dim, num_filt, kernels, drop),\n\u001B[32m     44\u001B[39m     \u001B[33m'\u001B[39m\u001B[33maccuracy\u001B[39m\u001B[33m'\u001B[39m: acc,\n\u001B[32m   (...)\u001B[39m\u001B[32m     47\u001B[39m     \u001B[33m'\u001B[39m\u001B[33mf1\u001B[39m\u001B[33m'\u001B[39m: f1\n\u001B[32m     48\u001B[39m })\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[12]\u001B[39m\u001B[32m, line 17\u001B[39m, in \u001B[36mtrain_model_with_early_stopping\u001B[39m\u001B[34m(model, train_loader, val_loader, criterion, optimizer, device, epochs, patience)\u001B[39m\n\u001B[32m     15\u001B[39m labels = batch[\u001B[33m'\u001B[39m\u001B[33mlabels\u001B[39m\u001B[33m'\u001B[39m].to(device)\n\u001B[32m     16\u001B[39m optimizer.zero_grad()\n\u001B[32m---> \u001B[39m\u001B[32m17\u001B[39m outputs = \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m=\u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     18\u001B[39m loss = criterion(outputs, labels)\n\u001B[32m     19\u001B[39m loss.backward()\n",
      "\u001B[36mFile \u001B[39m\u001B[32mF:\\UOJE\\Research\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mF:\\UOJE\\Research\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[8]\u001B[39m\u001B[32m, line 30\u001B[39m, in \u001B[36mSinbertCNN.forward\u001B[39m\u001B[34m(self, input_ids, attention_mask)\u001B[39m\n\u001B[32m     28\u001B[39m embeddings = outputs.last_hidden_state\n\u001B[32m     29\u001B[39m x = embeddings.permute(\u001B[32m0\u001B[39m, \u001B[32m2\u001B[39m, \u001B[32m1\u001B[39m)\n\u001B[32m---> \u001B[39m\u001B[32m30\u001B[39m conv_outputs = [torch.relu(\u001B[43mconv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m) \u001B[38;5;28;01mfor\u001B[39;00m conv \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.convs]\n\u001B[32m     31\u001B[39m pooled_outputs = [torch.max(conv_output, dim=\u001B[32m2\u001B[39m)[\u001B[32m0\u001B[39m] \u001B[38;5;28;01mfor\u001B[39;00m conv_output \u001B[38;5;129;01min\u001B[39;00m conv_outputs]\n\u001B[32m     32\u001B[39m concatenated_output = torch.cat(pooled_outputs, dim=\u001B[32m1\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mF:\\UOJE\\Research\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mF:\\UOJE\\Research\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32mF:\\UOJE\\Research\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:375\u001B[39m, in \u001B[36mConv1d.forward\u001B[39m\u001B[34m(self, input)\u001B[39m\n\u001B[32m    374\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) -> Tensor:\n\u001B[32m--> \u001B[39m\u001B[32m375\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_conv_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mF:\\UOJE\\Research\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:370\u001B[39m, in \u001B[36mConv1d._conv_forward\u001B[39m\u001B[34m(self, input, weight, bias)\u001B[39m\n\u001B[32m    358\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.padding_mode != \u001B[33m\"\u001B[39m\u001B[33mzeros\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m    359\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m F.conv1d(\n\u001B[32m    360\u001B[39m         F.pad(\n\u001B[32m    361\u001B[39m             \u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m._reversed_padding_repeated_twice, mode=\u001B[38;5;28mself\u001B[39m.padding_mode\n\u001B[32m   (...)\u001B[39m\u001B[32m    368\u001B[39m         \u001B[38;5;28mself\u001B[39m.groups,\n\u001B[32m    369\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m370\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[43m.\u001B[49m\u001B[43mconv1d\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    371\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdilation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mgroups\u001B[49m\n\u001B[32m    372\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mRuntimeError\u001B[39m: Given groups=1, weight of size [50, 1024, 3], expected input[16, 768, 128] to have 1024 channels, but got 768 channels instead"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T00:32:55.013169Z",
     "start_time": "2025-09-07T00:32:54.994260Z"
    }
   },
   "cell_type": "code",
   "source": [
    "Res=results.copy()\n",
    "Res"
   ],
   "id": "28c77cac1cb1a92",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'params': (768, 50, [3, 4, 5], 0.3),\n",
       "  'accuracy': 0.7131537242472267,\n",
       "  'precision': 0.7235713543282547,\n",
       "  'recall': 0.7131537242472267,\n",
       "  'f1': 0.7133121646035033},\n",
       " {'params': (768, 50, [3, 4, 5], 0.5),\n",
       "  'accuracy': 0.6988906497622821,\n",
       "  'precision': 0.6942882143258774,\n",
       "  'recall': 0.6988906497622821,\n",
       "  'f1': 0.6942250859933127},\n",
       " {'params': (768, 50, [2, 3, 4, 5], 0.3),\n",
       "  'accuracy': 0.722662440570523,\n",
       "  'precision': 0.7220994579815199,\n",
       "  'recall': 0.722662440570523,\n",
       "  'f1': 0.7177954903041444},\n",
       " {'params': (768, 50, [2, 3, 4, 5], 0.5),\n",
       "  'accuracy': 0.687797147385103,\n",
       "  'precision': 0.6901508621364858,\n",
       "  'recall': 0.687797147385103,\n",
       "  'f1': 0.688593325803551},\n",
       " {'params': (768, 100, [3, 4, 5], 0.3),\n",
       "  'accuracy': 0.7258320126782885,\n",
       "  'precision': 0.724638598002239,\n",
       "  'recall': 0.7258320126782885,\n",
       "  'f1': 0.7244884682376055},\n",
       " {'params': (768, 100, [3, 4, 5], 0.5),\n",
       "  'accuracy': 0.7068145800316957,\n",
       "  'precision': 0.7138818756657997,\n",
       "  'recall': 0.7068145800316957,\n",
       "  'f1': 0.7028937225101068},\n",
       " {'params': (768, 100, [2, 3, 4, 5], 0.3),\n",
       "  'accuracy': 0.722662440570523,\n",
       "  'precision': 0.7285341911263448,\n",
       "  'recall': 0.722662440570523,\n",
       "  'f1': 0.7170581021735253},\n",
       " {'params': (768, 100, [2, 3, 4, 5], 0.5),\n",
       "  'accuracy': 0.6957210776545166,\n",
       "  'precision': 0.704246463301379,\n",
       "  'recall': 0.6957210776545166,\n",
       "  'f1': 0.6973770210444664}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Validation",
   "id": "ff319dfb7ea45c07"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 5 fold cross validation",
   "id": "77b361a99457954b"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-09-07T14:39:11.116801Z"
    }
   },
   "cell_type": "code",
   "source": [
    "results_new=[]\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "k_folds = 5\n",
    "skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "labels = df['label'].values\n",
    "texts = df['text'].tolist()\n",
    "num_labels = 6\n",
    "fold_results = []\n",
    "train_losses_all = []\n",
    "val_losses_all = []\n",
    "model = SinbertCNN(\n",
    "        num_labels=num_labels,\n",
    "        embedding_dim=768,\n",
    "        num_filters=100,\n",
    "        kernel_sizes=[3,4,5],\n",
    "        dropout_rate=0.3\n",
    "    ).to(device)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(texts, labels)):\n",
    "    print(f\"Fold {fold+1}/{k_folds}\")\n",
    "    train_texts = [texts[i] for i in train_idx]\n",
    "    train_labels = [labels[i] for i in train_idx]\n",
    "    val_texts = [texts[i] for i in val_idx]\n",
    "    val_labels = [labels[i] for i in val_idx]\n",
    "\n",
    "    train_dataset = SinhalaEmotionDataset(train_texts, train_labels, tokenizer)\n",
    "    val_dataset = SinhalaEmotionDataset(val_texts, val_labels, tokenizer)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "    model = SinbertCNN(num_labels=len(np.unique(labels)), embedding_dim=768).to(device)\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "    criterion = nn.CrossEntropyLoss()  # Use class weights if needed\n",
    "\n",
    "    train_losses, train_accuracies, val_losses, val_accuracies = train_model_with_early_stopping(\n",
    "    model=model,\n",
    "    train_loader=train_data_loader,\n",
    "    val_loader=test_data_loader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    epochs=10,\n",
    "    patience=3\n",
    ")\n",
    "    train_losses_all.append(train_losses)\n",
    "    val_losses_all.append(val_losses)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "1b26972399492dca",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at NLPC-UOM/SinBERT-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at NLPC-UOM/SinBERT-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Train Loss: 1.6785, Train Acc: 0.3209, Val Loss: 1.2415, Val Acc: 0.5578\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "avg_train_loss = np.mean([np.array(l) for l in train_losses_all], axis=0)\n",
    "avg_val_loss = np.mean([np.array(l) for l in val_losses_all], axis=0)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(range(1, len(avg_train_loss)+1), avg_train_loss, label='Avg Train Loss')\n",
    "plt.plot(range(1, len(avg_val_loss)+1), avg_val_loss, label='Avg Val Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Average Training and Validation Loss Across Folds')\n",
    "plt.show()"
   ],
   "id": "62847e061e6d13cb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T14:35:12.080983Z",
     "start_time": "2025-09-07T14:35:11.883253Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'evaluate_metrics' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[13]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m final_res=[]\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m acc, precision, recall, f1 = \u001B[43mevaluate_metrics\u001B[49m(model, test_data_loader, device)\n\u001B[32m      3\u001B[39m final_res.append({\n\u001B[32m      4\u001B[39m     \u001B[33m'\u001B[39m\u001B[33maccuracy\u001B[39m\u001B[33m'\u001B[39m: acc,\n\u001B[32m      5\u001B[39m     \u001B[33m'\u001B[39m\u001B[33mprecision\u001B[39m\u001B[33m'\u001B[39m: precision,\n\u001B[32m      6\u001B[39m     \u001B[33m'\u001B[39m\u001B[33mrecall\u001B[39m\u001B[33m'\u001B[39m: recall,\n\u001B[32m      7\u001B[39m     \u001B[33m'\u001B[39m\u001B[33mf1\u001B[39m\u001B[33m'\u001B[39m: f1\n\u001B[32m      8\u001B[39m })\n",
      "\u001B[31mNameError\u001B[39m: name 'evaluate_metrics' is not defined"
     ]
    }
   ],
   "execution_count": 13,
   "source": [
    "final_res=[]\n",
    "acc, precision, recall, f1 = evaluate_metrics(model, test_data_loader, device)\n",
    "final_res.append({\n",
    "    'accuracy': acc,\n",
    "    'precision': precision,\n",
    "    'recall': recall,\n",
    "    'f1': f1\n",
    "})"
   ],
   "id": "32279b7be9e95e95"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
